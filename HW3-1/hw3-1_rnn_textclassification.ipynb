{"cells":[{"cell_type":"markdown","source":["## (*) **WRITE YOUR NAME**, course number and assignment number here\n","<br>  \n","<br>\n"],"metadata":{"id":"YLiPywWQB3UM"}},{"cell_type":"markdown","metadata":{"id":"2NXQYyrfNWwA"},"source":["# CSC 583 HW\\#3-1: Text Classification using RNNs and PyTorch (Fall 2025)\n","\n","This homework aims to give you hands-on experience of using the **PyTorch nn module**, in particular the Recurrent Neural Network (RNN) layers, to build a neural model in native PyTorch and to process natural language texts.\n","\n","The specific task is to classify text instances in a dataset containing news articles on Covid-19 into True or Fake.  It is a binary classification, and you are provided with an annotated training set (with ground-truth labels) and a test set (with no labels).\n","\n","**Your task is to fill in the sections indicated in the code with \"TO-DO(n)\" -- where 0 <= n <= 11.**\n","\n","**After you fill in all sections, rename the file for your homework submission** -- REQUIRED (and will be due for point deduction if not done)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uLbx0B8TpaRi"},"outputs":[],"source":["## Code piece to mount my Google Drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\") # my Google Drive root directory will be mapped here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhEk4t57qhfi"},"outputs":[],"source":["## CHANGE the working directory to YOUR own work directory (where the code file is).\n","import os\n","thisdir = '/content/drive/My Drive/CSC583_2025Fall/HW3-1'\n","os.chdir(thisdir)\n","\n","# Ensure the files are there (in the folder)\n","!pwd"]},{"cell_type":"markdown","source":["**IMPORTANT HINT**: Consult the baked-in output ([\"583_HW3-1_startup.pdf\"](https://drive.google.com/file/d/1GHXvdWw1oO5r77HqXy1SLosnIYGmGIWm/view?usp=drive_link) ) to verify the correctness as you develop your code.\n","\n","\n","=================================================================================================\n"],"metadata":{"id":"J_F6VFeys7CA"}},{"cell_type":"markdown","metadata":{"id":"RlyG3mVZNWwF"},"source":["## Check for GPU's"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7haUEK2eNWwF"},"outputs":[],"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():\n","    # Tell PyTorch to use the GPU.\n","    device = torch.device(\"cuda\")\n","    print ('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print ('We will use the GPU:', torch.cuda.get_device_name(0))\n","# If not...\n","else:\n","    print ('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"F2hpjBLbNWwG"},"source":["## Some important import's"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTxtA5V2NWwG"},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import string\n","import re\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from torch.utils.data import TensorDataset, DataLoader"]},{"cell_type":"markdown","metadata":{"id":"RdsOshQMzmgu"},"source":["## (1) Load datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"2asoR808NWwH"},"outputs":[],"source":["# TO-DO (0):\n","# Load train and test data into pandas dataframe (separately).\n","# You can upload the data files in your Colab folder ('thisdir'), or the files are\n","# accessible on Github, https://github.com/ntomuro/CSC583_2025Fall/tree/main/HW3-1\n","# Be sure to specify the encoding to be 'utf-8' and the # delimiter to be '\\t' (tab).\n","\n","\n","\n","\n","\n","\n","\n","#----------------\n","# a test call\n","print (df_train.shape)\n","print (df_test.shape)\n","df_train.tail()"]},{"cell_type":"markdown","metadata":{"id":"s89ma8QBNWwH"},"source":["### Inspect some properties of the datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RH8tqVLWNWwH"},"outputs":[],"source":["# Check the class distribution (Fake/True; stratified) in train and test sets\n","def binary_ratio(df):\n","    shape = df.shape\n","    fakecount = df[(df['labels'] == 0)].shape[0]  # count of fake entries\n","    print (f'Fake ratio: shape={df.shape} -- fake {fakecount}/{shape[0]} = {fakecount/shape[0]}')\n","\n","binary_ratio(df_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X7GkONRcNWwI"},"outputs":[],"source":["import statistics\n","\n","# sentence lengths for the training set\n","df_train['content_length'] = df_train['content'].apply(lambda x: len(x.split())) # simple white-space delimiter\n","\n","# mean and stdev of lengths\n","lengths = df_train['content_length'].tolist()\n","print (f'Mean: {statistics.mean(lengths)}, Stdev: {statistics.stdev(lengths)}, Max: {max(lengths)}')\n","\n","df_train.head()"]},{"cell_type":"markdown","metadata":{"id":"fmWe6_UbNWwI"},"source":["## (2) Build vocabulary\n","\n","We build the vocabulary from words/tokens in the training set.\n","\n","First we define/obtain a tokenizer.  We will use a simple white-space-based tokenizer (used in GloVe), which is essentially what NLTK's word_tokenize() does.  After tokenization, we will convert text into <b>lower case</b> and <b>remove punctuations and numbers</b> in addition."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PXqCiUKUNWwI"},"outputs":[],"source":["!pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"qkRab8LANWwI"},"outputs":[],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yx5cCdy7NWwI"},"outputs":[],"source":["# Check NLTK's word_tokenize() function.\n","from nltk import word_tokenize\n","\n","sent = \"Congress has passed a US$8.3 billion coronavirus response bill, which includes $2.2 billion for the CDC to â€œprevent, prepare for, and respond to coronavirus, domestically or internationally.â€\"\n","#\"Text classification is a fundamental natural language processing (NLP) task.\"\n","\n","tokens = word_tokenize(sent)\n","print (tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4FFMpEtlNWwJ"},"outputs":[],"source":["# Our tokenizer function\n","def tokenize (text):\n","    # first clean up the text by replacing non-ascii characters to a space\n","    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n","\n","    # TO-DO (1):\n","    # Continue to tokenize text. You do these in ANY ORDER: 1. removing punctuations,\n","    # 2. removing numbers, 3. changing text to lower case, 4. tokenize text into tokens\n","    # (by word_tokenize()).  Return the tokens in a list.\n","\n","\n","\n","\n","\n","#----------------\n","# a test call\n","print (tokenize(sent))"]},{"cell_type":"markdown","metadata":{"id":"0brM-7e5NWwJ"},"source":["### Tokenize each text and save results in a new column 'content_tokenized' in the dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nsu9G-nFNWwJ"},"outputs":[],"source":["# TO-DO (2):\n","# Apply tokenizer to each text in 'content' in df_train.\n","# Store the results in a new column 'content_tokenized'.\n","# (*) Be sure to nest the output in np.array (to make a list of one element)\n","# because pandas df does not accept arrays of different length (i.e., jagged\n","# arrays) or arrays of strings (in our case, tokens).\n","\n","\n","\n","\n","\n","#----------------\n","# a test call\n","df_train.head()"]},{"cell_type":"markdown","metadata":{"id":"UMnWHBcmNWwJ"},"source":["### Collect tokens and store them in NLTK's FreqDist dictionary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fAbGgyetNWwJ"},"outputs":[],"source":["# function to flatten a nested list to a flat list\n","def flatten(sents):\n","  # assuming the nesting level of 2..\n","  return [token for sent in sents for token in sent]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Jn06NOZNWwJ"},"outputs":[],"source":["# Collect tokenized results into a list\n","all_tokens_list = [wlist[0] for wlist in df_train['content_tokenized'].tolist()]\n","token_list = flatten(all_tokens_list)\n","\n","# NLTK's FreqDist\n","fdist = nltk.probability.FreqDist(token_list)\n","print (fdist)"]},{"cell_type":"markdown","metadata":{"id":"xpTYVN33NWwJ"},"source":["### Finalize vocabulary as tokens that occurred >= 2 times, plus \"\" and 'UNK'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D1ACvRB8NWwJ"},"outputs":[],"source":["# TO-DO (3):\n","# Select tokens that appeared >= 2 times, and sort them.\n","# Merge them with the list [\"\", 'UNK'].  Note \"\" for padding\n","# and 'UNK' for unknown tokens.  Name the final vocabulary as 'voc'.\n","\n","\n","\n","\n","\n","\n","\n","#----------------\n","# a test call\n","vocab_size = len(vocab) # this variable will be used later\n","print (f'vocabulary_size: {vocab_size}')\n","print (vocab[:10])"]},{"cell_type":"markdown","metadata":{"id":"MLM42c0pNWwK"},"source":["### Create vocabulary lookup tables as well"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQQozQ48NWwK"},"outputs":[],"source":["# Vocabulary lookup tables\n","vocab2index = {} # token to index lookup\n","index2vocab = {} # index to token (reverse) lookup\n","\n","for idx, token in enumerate(vocab):\n","    vocab2index[token] = idx\n","    index2vocab[idx] = token"]},{"cell_type":"markdown","metadata":{"id":"b7xcZm7rNWwK"},"source":["### Encode each text (token -> idex) and save results in a new column in the dataframe.\n","Text is truncated to the maximum input length (<b>max_input_len</b>).  Also, tokens that are not in the vocabulary are indicated with 'UNK'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Avrqm5lTNWwK"},"outputs":[],"source":["max_input_len = 50 # this variable will be used later too\n","\n","# Returns a numpy array of tokens of a _fixed_ size (N -- defaults to 'max_input_len')\n","def encode_sentence(tokenized_text, vocab2index, N=max_input_len):\n","    # TO-DO (4):\n","    # Create a list of vocabulary indices for the tokens in 'tokenized_text' (NOT nested)\n","    # (e.g. ['a' 'video' 'showing' 'an' 'anti' 'china' 'protest' 'amid' 'the'..])\n","    # and return the list (in a non-nested, fixed size (N) numpy array).\n","    # Assume the token indices are recorded in 'vocab2index' dictionary.\n","    # (*) If a token is not in the vocabulary, the index associated with 'UNK'\n","    # should be selected for the token.\n","    # (*) If the length of the 'tokenized_text' is longer than N, it will be truncated.\n","    # Or if the length is shorter, the remaining slots in the resulting index list\n","    # should be padded with 0's.\n","    # Return the vocabulary index list (i.e., encoded list) and it's length.\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o9pMo1iRNWwK"},"outputs":[],"source":["# each entry in df_train['encoded'] has the same/fixed length of 'max_input_len' (and the\n","#  remainders are filled with index 0 -- the padding character)\n","df_train['encoded'] = df_train['content_tokenized'].apply(lambda x: encode_sentence(x.tolist()[0], vocab2index)[0])\n","print (f'{df_train.loc[2].encoded}, \\n{df_train.loc[2].content_tokenized}')\n","print (f'{df_train.loc[4].encoded}, \\n{df_train.loc[4].content_tokenized}')\n","df_train.head()"]},{"cell_type":"markdown","metadata":{"id":"Y0U7Dl_TNWwK"},"source":["### (**) Do the same preprocessing steps for the <u>test set</u> (using the vocabulary constructed from the training set)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KtYSVxtWNWwK"},"outputs":[],"source":["# TO-DO (5):\n","# First obtain sentence lengths for each content entry and assign to a new column 'content_length'\n","\n","\n","\n","\n","# Next tokenize each content and save the tokenized tokens in a new column 'content_tokenized'\n","\n","\n","\n","# Then encode the text (into indices)\n","\n","\n","\n","\n","\n","#----------------\n","# a test call\n","df_test.head()"]},{"cell_type":"markdown","metadata":{"id":"GeihzdvMNWwK"},"source":["## (3) Create PyTorch Datasets and DataLoaders\n","We first define a custom 'MyDataset' class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JFTzFpZ7NWwL"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class MyDataset(Dataset):\n","    def __init__(self, X, Y):\n","        self.X = X\n","        self.y = Y\n","\n","    def __len__(self):\n","        return len(self.y)\n","\n","    def __getitem__(self, idx):\n","        # returns a torch tensor (possibly from a numpy array)\n","        return torch.from_numpy(self.X[idx].astype(np.int32)), self.y[idx]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sxNlG8FuNWwL"},"outputs":[],"source":["# Prepare train/validation/test data\n","x = df_train['encoded'].tolist()\n","y = df_train['labels'].tolist()\n","x_test = df_test['encoded'].tolist()\n","\n","# TO-DO (6):\n","# Split the training ('x' and 'y' above) into train and validation, with 20% for validation.\n","# Be sure to split using stratification.\n","# Name the resulting variables as x_train, x_valid, y_train, y_valid.\n","\n","\n","\n","\n","\n","#----------------\n","# a test call\n","print (f'Training contains {len(x_train)} instances; Validation contains {len(x_valid)} instances')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1kvMNZANWwL"},"outputs":[],"source":["# Then create custom Datasets\n","train_ds = MyDataset(x_train, y_train)\n","valid_ds = MyDataset(x_valid, y_valid)"]},{"cell_type":"markdown","metadata":{"id":"rbQOyWZbNWwL"},"source":["### (*) Dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C6zCw2XnNWwL"},"outputs":[],"source":["batch_size = 64\n","\n","train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","valid_dataloader = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"q9qPGDjqNWwL"},"source":["## (4) Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pNzYguP7NWwL"},"outputs":[],"source":["class MyLSTM(torch.nn.Module) :\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers=1,\n","                 bidirectional=False, dropout=0.0):\n","        super().__init__()\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n","                           bidirectional=bidirectional, dropout=dropout, batch_first=True)\n","        self.linear = nn.Linear(hidden_dim, output_dim)\n","        # activation function for the output layer -- for binary/logistic classification\n","        self.act = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.embeddings(x)\n","        lstm_out, (ht, ct) = self.lstm(x)\n","        out = self.linear(ht[-1])\n","        return self.act(out)"]},{"cell_type":"markdown","metadata":{"id":"j4sOyjcJNWwU"},"source":["## (5) Training -- train and eval functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OT2HtOLXKvIc"},"outputs":[],"source":["# function to predict accuracy (or number of correctly classified instances)\n","def acc(pred,label):\n","    pred = torch.round(pred.squeeze())\n","    return torch.sum(pred == label.squeeze()).item()\n","\n","#----------------------------\n","# function to train the model\n","#----------------------------\n","def train_model(model, epochs=10, lr=0.001, weight_decay=1e-5):\n","    # define optimizer (Adam, for parameters that require gradient)\n","    parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    optimizer = torch.optim.Adam(parameters, lr=lr, weight_decay=weight_decay)\n","    #\n","    valid_loss_min = np.Inf\n","\n","    # save the initial model\n","    torch.save(model.state_dict(), init_model_path) # current best model\n","\n","    ## training loop - for each epoch\n","    for epoch in range(epochs):\n","        ##======== (1) Training ========\n","        # (*) set the mode to train\n","        model.train()\n","        # results accumulator variables\n","        train_losses = [] # trace of losses (over batches)\n","        train_acc = 0.0   # total number of correctly classified instances\n","\n","        # iterate over mini-batches\n","        for inputs, labels in train_dataloader:\n","            # push them to the GPU\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            # (*) clear the gradients\n","            optimizer.zero_grad()\n","            # forward propagate to obtain prediction\n","            output = model(inputs)\n","\n","            # compute loss\n","            loss = criterion(output.squeeze(), labels.float())\n","            # backward propagation\n","            loss.backward()\n","\n","            # record the loss (by appending the value to the list of losses)\n","            train_losses.append(loss.item())\n","            # calculating accuracy (accumulate correct count)\n","            accuracy = acc(output,labels)\n","            train_acc += accuracy\n","\n","            #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","            nn.utils.clip_grad_norm_(model.parameters(), clip)\n","            # update the weights\n","            optimizer.step()\n","\n","        ##======== (2) Evaluation ========\n","        val_losses, val_acc = evaluate(model, valid_dataloader)\n","\n","        ##======== (3) Reporting ========\n","        epoch_train_loss = np.mean(train_losses)\n","        epoch_val_loss = np.mean(val_losses)\n","        epoch_train_acc = train_acc/len(train_dataloader.dataset)\n","        epoch_val_acc = val_acc/len(valid_dataloader.dataset)\n","        epoch_tr_loss.append(epoch_train_loss)\n","        epoch_vl_loss.append(epoch_val_loss)\n","        epoch_tr_acc.append(epoch_train_acc)\n","        epoch_vl_acc.append(epoch_val_acc)\n","        print(f'Epoch {epoch+1}')\n","        print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n","        print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n","\n","        if epoch_val_loss <= valid_loss_min:\n","            torch.save(model.state_dict(), best_model_path) # current best model\n","            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n","            valid_loss_min = epoch_val_loss\n","        print(25*'==')\n","\n","#----------------------------\n","# function to evaluate the model\n","#----------------------------\n","def evaluate(model, valid_dl):\n","    # (*) set the mode to evaluation\n","    model.eval()\n","    #\n","    val_losses = [] # trace of losses (over batches)\n","    val_acc = 0.0   # total number of correctly classified instances\n","\n","    #deactivate autograd since it's not needed during evaluation\n","    with torch.no_grad():\n","        # TO-DO (7):\n","        # Evaluate the model with respect to the validation dataset ('valid_dl').\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    return val_losses, val_acc\n"]},{"cell_type":"markdown","metadata":{"id":"pQv99AZSNWwU"},"source":["### Define the loss function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LiMoBFnvNWwU"},"outputs":[],"source":["import torch.optim as optim\n","\n","#define the loss function -- binary cross-entropy\n","# (https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss)\n","criterion = nn.BCELoss()\n","\n","#push to cuda if available\n","criterion = criterion.to(device)"]},{"cell_type":"markdown","metadata":{"id":"tvLP2pvcNWwV"},"source":["### Create a model and set up other parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VsLxA12uNWwV"},"outputs":[],"source":["embedding_dim = 50\n","output_dim = 1\n","hidden_dim = 128\n","clip = 5\n","\n","# train for some number of epochs\n","epoch_tr_loss,epoch_vl_loss = [],[]\n","epoch_tr_acc,epoch_vl_acc = [],[]\n","\n","# paths to save models\n","init_model_path ='./saved/init_model.pt'\n","best_model_path ='./saved/best_state_model.pt'\n","\n","# Create a model\n","model = MyLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.2)\n","\n","#moving the model to gpu\n","model.to(device)\n","\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"RuVba6EvNWwV"},"source":["### Finally train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PivPs8EZNWwV"},"outputs":[],"source":["# TO-DO (8):\n","# Try various number of epochs and lr, as well as model architecture\n","# parameters (e.g. number of layers, bidirectional, recurrent drop-out etc.)\n","train_model(model, epochs=200, lr=0.0005)\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TSmDTdvlNWwV"},"source":["### Visualize training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2EEkMb2ztmi"},"outputs":[],"source":["# TO-DO (9):\n","# Visualize the training results.  You can do more epochs, but be sure to\n","# compare training and validation accuracies and losses.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TOJ27RNb0e4T","outputId":"6b6a77c3-e759-4f4d-e89b-61abf4ed7bdc"},"source":["##  (6) Prediction/Inference with the test set\n","### First load the <b>saved best model</b> and define the inference function that accepts the user defined input and make predictions ('./RNN-references/Text%20Classification%20Pytorch%20_%20Build%20Text%20Classification%20Model.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jjat9kKv0ihc"},"outputs":[],"source":["#load weights from the saved best model\n","model = MyLSTM(vocab_size, embedding_dim, hidden_dim, output_dim)\n","model.to(device)\n","\n","model.load_state_dict(torch.load(best_model_path))\n","model.eval()  # set the mode to eval (i.e., no gradient)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejFsts7P0l-b"},"outputs":[],"source":["#----------------------------\n","# function to generate predictions for the testset\n","#----------------------------\n","def predict(model, test_list):\n","    # (*) set the mode to evaluation\n","    model.eval()\n","    #\n","    prediction_list = [] # store predictions\n","\n","    with torch.no_grad(): #deactivates autograd\n","        # TO-DO (10):\n","        # Obtain prediction for each instance in the test set/list\n","        # and accumulate them in 'prediction_list'.\n","\n","\n","\n","\n","\n","\n","    # Return 'prediction_list'\n","    return prediction_list\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8XanmGA0s94"},"outputs":[],"source":["##-------------------------------\n","## Inference/generate predictions\n","##-------------------------------\n","predictions = predict(model, x_test)\n"]},{"cell_type":"markdown","metadata":{"id":"fe0fFT1dNWwW"},"source":["### Write test predictions to a csv file (for Kaggle submission)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DWSSlnny0p6K"},"outputs":[],"source":["# TO-DO (11):\n","# Write your own code.\n","\n","\n","\n","\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}